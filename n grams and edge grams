
big data processing with spark and scala

[big,data,processing,with,spark,scala]

pro--> will not fetch any result

n gram
word=search
len=1 [s,e,a,r,c,h]
len=2 [se,ea,rc,ch]

edge grams
word=search
s
se
sea
sear
search


DELETE /blogposts


PUT /blogposts
{
  "settings": {
    "analysis": {
      
      "analyzer": {
        "my_custom_analyzer":
        {
          "type":"custom",
          "char_filter":["html_strip"],
          "tokenizer":"standard",
          "fliter":["lowercase"]
        }
        
      }
    }
  }
  
  , "mappings": {
    "properties":
    {
      "title":{"type":"text"},
      "content":{"type":"text","analyzer":"my_custom_analyzer"},
      "published_data":{"type":"date"}
    }
  }
  
}

GET /blogposts

# to check how inverted indexs
POST /blogposts/_analyze
{
  "analyzer": "my_custom_analyzer",
  "text":"this is html <p> word "
}





PUT /blogposts
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_analyzer":{
          "type":"custom",
          "tokenizer":"punctuation",
          "char_filter":["symbol"],
          "filter":["my_stop","lowercase"]
          
        }
      },
      "tokenizer": {
        "punctuation":{
          "type":"pattern",
          "pattern":"[ .!,?]"
        }
      },
      "char_filter": {
        "symbol":
        {
          "type":"mapping",
          "mappings":["& => and",":) => happy",":( => sad "]
        }
      }
      , "filter": {
        "my_stop":{
          "type":"stop",
          "stopwords":"_english_"
        }
      }
    }
  }
}

# and is removed

# to check how inverted indexs
POST /blogposts/_analyze
{
  "analyzer": "my_custom_analyzer",
  "text":"Big Data processing using spark & scala :)"
}


POST /_analyze
{
  "tokenizer": "ngram",
  "text": "search"
}


PUT ngram_index
{
  "settings": {
    "analysis": {
      
      "analyzer": {
        "ngram_analyzer":
        {
          "tokenizer":"ngram_tokenizer"
        }
      }
      
      , "tokenizer": {
        "ngram_tokenizer":
        {
          "type":"ngram",
          "min_gram":2,
          "max_gram":3
        }
      }
    }
  }
}


POST /ngram_index/_analyze
{
  "analyzer": "ngram_analyzer",
  "text":"search"
}

DELETE /edge_gram


PUT edge_gram
{
  "settings": {
    "analysis": {
      
      "analyzer": {
        "edge_gram_analyzer":
        {
          "tokenizer":"edge_tokenizer"
        }
      }
      
      , "tokenizer": {
        "edge_tokenizer":
        {
          "type":"edge_ngram",
          "min_gram":2,
          "max_gram":100
        }
      }
      
    }
  }
}

POST /edge_gram/_analyze
{
  "analyzer": "edge_gram_analyzer",
  "text": "search"
}












